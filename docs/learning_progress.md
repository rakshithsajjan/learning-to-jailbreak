# Learning Progress Tracker

## Proficiency Levels
- 🌱 **Novice**: Just introduced, needs basic explanations
- 🌿 **Beginner**: Understands concept, needs practice
- 🌳 **Intermediate**: Can apply concept, occasional guidance needed
- 🌲 **Proficient**: Comfortable using, minimal help needed
- 🎄 **Expert**: Can teach others, innovate

## ML Fundamentals
### Basic Concepts
- [ ] 🌱 Supervised vs Unsupervised Learning
- [ ] 🌱 Training vs Testing Data
- [ ] 🌱 Overfitting/Underfitting
- [ ] 🌱 Loss Functions
- [ ] 🌱 Metrics (Accuracy, Precision, Recall)

### Mathematics
#### Linear Algebra
- [ ] 🌱 Vectors and Matrices
- [ ] 🌱 Matrix Multiplication
- [ ] 🌱 Dot Products
- [ ] 🌱 Eigenvalues/Eigenvectors

#### Calculus
- [ ] 🌱 Derivatives
- [ ] 🌱 Partial Derivatives
- [ ] 🌱 Chain Rule
- [ ] 🌱 Gradients

#### Probability & Statistics
- [ ] 🌱 Probability Distributions
- [ ] 🌱 Bayes' Theorem
- [ ] 🌱 Expected Value
- [ ] 🌱 Variance and Standard Deviation

## Deep Learning
### Neural Network Basics
- [ ] 🌱 Perceptron
- [ ] 🌱 Activation Functions
  - [ ] 🌱 Sigmoid
  - [ ] 🌱 ReLU
  - [ ] 🌱 Tanh
  - [ ] 🌱 Softmax
- [ ] 🌱 Forward Propagation
- [ ] 🌱 Backpropagation

### Gradient Descent
- [ ] 🌱 Basic Gradient Descent
  - [ ] 🌱 Learning Rate
  - [ ] 🌱 Cost Function Minimization
- [ ] 🌱 Stochastic Gradient Descent (SGD)
- [ ] 🌱 Mini-batch Gradient Descent
- [ ] 🌱 Advanced Optimizers
  - [ ] 🌱 Momentum
  - [ ] 🌱 Adam
  - [ ] 🌱 RMSprop

### Training Techniques
- [ ] 🌱 Batch Normalization
- [ ] 🌱 Dropout
- [ ] 🌱 Early Stopping
- [ ] 🌱 Learning Rate Scheduling

## Language Models
### Transformer Architecture
- [ ] 🌱 Attention Mechanism
  - [ ] 🌱 Self-Attention
  - [ ] 🌱 Multi-Head Attention
  - [ ] 🌱 Positional Encoding
- [ ] 🌱 Encoder-Decoder Structure
- [ ] 🌱 Tokenization
  - [ ] 🌱 BPE
  - [ ] 🌱 WordPiece
  - [ ] 🌱 SentencePiece

### Pre-trained Models
- [ ] 🌱 GPT Architecture
- [ ] 🌱 BERT vs GPT
- [ ] 🌱 Fine-tuning Concepts
- [ ] 🌱 Prompt Engineering
- [ ] 🌱 In-Context Learning

### LLM Training
- [ ] 🌱 Pretraining
- [ ] 🌱 Supervised Fine-tuning (SFT)
- [ ] 🌱 Instruction Tuning
- [ ] 🌱 Constitutional AI

## Reinforcement Learning
### Core Concepts
- [ ] 🌱 Agent-Environment Interaction
- [ ] 🌱 States, Actions, Rewards
- [ ] 🌱 Episodes and Trajectories
- [ ] 🌱 Markov Decision Process (MDP)

### Value-Based Methods
- [ ] 🌱 Value Functions
- [ ] 🌱 Q-Learning
- [ ] 🌱 Deep Q-Networks (DQN)
- [ ] 🌱 Bellman Equation

### Policy-Based Methods
- [ ] 🌱 Policy Gradient
- [ ] 🌱 REINFORCE Algorithm
- [ ] 🌱 Actor-Critic Methods

### Advanced RL
- [ ] 🌱 Proximal Policy Optimization (PPO)
  - [ ] 🌱 Clipping Objective
  - [ ] 🌱 KL Divergence Constraint
- [ ] 🌱 Trust Region Policy Optimization (TRPO)
- [ ] 🌱 Direct Preference Optimization (DPO)
- [ ] 🌱 Group Robust Policy Optimization (GRPO)

### RL for LLMs
- [ ] 🌱 Reward Modeling
- [ ] 🌱 RLHF (RL from Human Feedback)
- [ ] 🌱 Constitutional RLHF
- [ ] 🌱 Reward Hacking Prevention

## Adversarial ML
### Attack Methods
- [ ] 🌱 Adversarial Examples
- [ ] 🌱 Gradient-Based Attacks
- [ ] 🌱 Black-Box vs White-Box Attacks
- [ ] 🌱 Transferability

### LLM-Specific Attacks
- [ ] 🌱 Prompt Injection
- [ ] 🌱 Jailbreaking Techniques
  - [ ] 🌱 Role-playing
  - [ ] 🌱 Hypotheticals
  - [ ] 🌱 Encoding/Obfuscation
  - [ ] 🌱 Multi-turn Strategies
- [ ] 🌱 Universal Adversarial Triggers
- [ ] 🌱 Gradient-based Prompt Optimization

### Defense Mechanisms
- [ ] 🌱 Input Filtering
- [ ] 🌱 Output Monitoring
- [ ] 🌱 Adversarial Training
- [ ] 🌱 Certified Defenses

## Programming Skills
### Python
- [ ] 🌱 Basic Syntax
- [ ] 🌱 NumPy Arrays
- [ ] 🌱 Pandas DataFrames
- [ ] 🌱 Matplotlib/Seaborn Plotting
- [ ] 🌱 Object-Oriented Programming
- [ ] 🌱 Debugging Techniques

### PyTorch
- [ ] 🌱 Tensors
- [ ] 🌱 Autograd
- [ ] 🌱 nn.Module
- [ ] 🌱 DataLoaders
- [ ] 🌱 Training Loops
- [ ] 🌱 Model Checkpointing

### ML Libraries
- [ ] 🌱 Hugging Face Transformers
- [ ] 🌱 Weights & Biases (Logging)
- [ ] 🌱 Ray/RLlib (for RL)
- [ ] 🌱 LangChain (for LLM apps)

## Research Skills
### Literature Review
- [ ] 🌱 Finding Relevant Papers
- [ ] 🌱 Reading Research Papers
- [ ] 🌱 Summarizing Key Ideas
- [ ] 🌱 Critical Analysis

### Experimentation
- [ ] 🌱 Hypothesis Formation
- [ ] 🌱 Experimental Design
- [ ] 🌱 Ablation Studies
- [ ] 🌱 Statistical Significance

### Scientific Writing
- [ ] 🌱 Abstract Writing
- [ ] 🌱 Related Work Section
- [ ] 🌱 Methodology Description
- [ ] 🌱 Results Presentation
- [ ] 🌱 LaTeX Basics

## Project-Specific Skills
### Jailbreak Research
- [ ] 🌱 Safety Evaluation Metrics
- [ ] 🌱 Attack Success Measurement

### Implementation
- [ ] 🌱 Setting up LLM APIs
- [ ] 🌱 Prompt Template Design
- [ ] 🌱 Batch Generation
- [ ] 🌱 Result Analysis

---

## Update Log
### 2025-07-14
- Initial progress tracker created
- All topics marked as 🌱 Novice to start

### How to Update
When you learn something new or improve your understanding:
1. Change the emoji to reflect new proficiency level
2. Add date and what you learned in the update log
3. This helps track learning journey and adjust guidance

### Next Focus Areas
Based on project needs, prioritize:
1. Python basics if not comfortable
2. Understanding jailbreaking conceptually
3. Basic neural networks
4. Simple RL concepts