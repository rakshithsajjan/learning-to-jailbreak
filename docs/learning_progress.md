# Learning Progress Tracker

## Proficiency Levels
- ğŸŒ± **Novice**: Just introduced, needs basic explanations
- ğŸŒ¿ **Beginner**: Understands concept, needs practice
- ğŸŒ³ **Intermediate**: Can apply concept, occasional guidance needed
- ğŸŒ² **Proficient**: Comfortable using, minimal help needed
- ğŸ„ **Expert**: Can teach others, innovate

## ML Fundamentals
### Basic Concepts
- [ ] ğŸŒ± Supervised vs Unsupervised Learning
- [ ] ğŸŒ± Training vs Testing Data
- [ ] ğŸŒ± Overfitting/Underfitting
- [ ] ğŸŒ± Loss Functions
- [ ] ğŸŒ± Metrics (Accuracy, Precision, Recall)

### Mathematics
#### Linear Algebra
- [ ] ğŸŒ± Vectors and Matrices
- [ ] ğŸŒ± Matrix Multiplication
- [ ] ğŸŒ± Dot Products
- [ ] ğŸŒ± Eigenvalues/Eigenvectors

#### Calculus
- [ ] ğŸŒ± Derivatives
- [ ] ğŸŒ± Partial Derivatives
- [ ] ğŸŒ± Chain Rule
- [ ] ğŸŒ± Gradients

#### Probability & Statistics
- [ ] ğŸŒ± Probability Distributions
- [ ] ğŸŒ± Bayes' Theorem
- [ ] ğŸŒ± Expected Value
- [ ] ğŸŒ± Variance and Standard Deviation

## Deep Learning
### Neural Network Basics
- [ ] ğŸŒ± Perceptron
- [ ] ğŸŒ± Activation Functions
  - [ ] ğŸŒ± Sigmoid
  - [ ] ğŸŒ± ReLU
  - [ ] ğŸŒ± Tanh
  - [ ] ğŸŒ± Softmax
- [ ] ğŸŒ± Forward Propagation
- [ ] ğŸŒ± Backpropagation

### Gradient Descent
- [ ] ğŸŒ± Basic Gradient Descent
  - [ ] ğŸŒ± Learning Rate
  - [ ] ğŸŒ± Cost Function Minimization
- [ ] ğŸŒ± Stochastic Gradient Descent (SGD)
- [ ] ğŸŒ± Mini-batch Gradient Descent
- [ ] ğŸŒ± Advanced Optimizers
  - [ ] ğŸŒ± Momentum
  - [ ] ğŸŒ± Adam
  - [ ] ğŸŒ± RMSprop

### Training Techniques
- [ ] ğŸŒ± Batch Normalization
- [ ] ğŸŒ± Dropout
- [ ] ğŸŒ± Early Stopping
- [ ] ğŸŒ± Learning Rate Scheduling

## Language Models
### Transformer Architecture
- [ ] ğŸŒ± Attention Mechanism
  - [ ] ğŸŒ± Self-Attention
  - [ ] ğŸŒ± Multi-Head Attention
  - [ ] ğŸŒ± Positional Encoding
- [ ] ğŸŒ± Encoder-Decoder Structure
- [ ] ğŸŒ± Tokenization
  - [ ] ğŸŒ± BPE
  - [ ] ğŸŒ± WordPiece
  - [ ] ğŸŒ± SentencePiece

### Pre-trained Models
- [ ] ğŸŒ± GPT Architecture
- [ ] ğŸŒ± BERT vs GPT
- [ ] ğŸŒ± Fine-tuning Concepts
- [ ] ğŸŒ± Prompt Engineering
- [ ] ğŸŒ± In-Context Learning

### LLM Training
- [ ] ğŸŒ± Pretraining
- [ ] ğŸŒ± Supervised Fine-tuning (SFT)
- [ ] ğŸŒ± Instruction Tuning
- [ ] ğŸŒ± Constitutional AI

## Reinforcement Learning
### Core Concepts
- [ ] ğŸŒ± Agent-Environment Interaction
- [ ] ğŸŒ± States, Actions, Rewards
- [ ] ğŸŒ± Episodes and Trajectories
- [ ] ğŸŒ± Markov Decision Process (MDP)

### Value-Based Methods
- [ ] ğŸŒ± Value Functions
- [ ] ğŸŒ± Q-Learning
- [ ] ğŸŒ± Deep Q-Networks (DQN)
- [ ] ğŸŒ± Bellman Equation

### Policy-Based Methods
- [ ] ğŸŒ± Policy Gradient
- [ ] ğŸŒ± REINFORCE Algorithm
- [ ] ğŸŒ± Actor-Critic Methods

### Advanced RL
- [ ] ğŸŒ± Proximal Policy Optimization (PPO)
  - [ ] ğŸŒ± Clipping Objective
  - [ ] ğŸŒ± KL Divergence Constraint
- [ ] ğŸŒ± Trust Region Policy Optimization (TRPO)
- [ ] ğŸŒ± Direct Preference Optimization (DPO)
- [ ] ğŸŒ± Group Robust Policy Optimization (GRPO)

### RL for LLMs
- [ ] ğŸŒ± Reward Modeling
- [ ] ğŸŒ± RLHF (RL from Human Feedback)
- [ ] ğŸŒ± Constitutional RLHF
- [ ] ğŸŒ± Reward Hacking Prevention

## Adversarial ML
### Attack Methods
- [ ] ğŸŒ± Adversarial Examples
- [ ] ğŸŒ± Gradient-Based Attacks
- [ ] ğŸŒ± Black-Box vs White-Box Attacks
- [ ] ğŸŒ± Transferability

### LLM-Specific Attacks
- [ ] ğŸŒ± Prompt Injection
- [ ] ğŸŒ± Jailbreaking Techniques
  - [ ] ğŸŒ± Role-playing
  - [ ] ğŸŒ± Hypotheticals
  - [ ] ğŸŒ± Encoding/Obfuscation
  - [ ] ğŸŒ± Multi-turn Strategies
- [ ] ğŸŒ± Universal Adversarial Triggers
- [ ] ğŸŒ± Gradient-based Prompt Optimization

### Defense Mechanisms
- [ ] ğŸŒ± Input Filtering
- [ ] ğŸŒ± Output Monitoring
- [ ] ğŸŒ± Adversarial Training
- [ ] ğŸŒ± Certified Defenses

## Programming Skills
### Python
- [ ] ğŸŒ± Basic Syntax
- [ ] ğŸŒ± NumPy Arrays
- [ ] ğŸŒ± Pandas DataFrames
- [ ] ğŸŒ± Matplotlib/Seaborn Plotting
- [ ] ğŸŒ± Object-Oriented Programming
- [ ] ğŸŒ± Debugging Techniques

### PyTorch
- [ ] ğŸŒ± Tensors
- [ ] ğŸŒ± Autograd
- [ ] ğŸŒ± nn.Module
- [ ] ğŸŒ± DataLoaders
- [ ] ğŸŒ± Training Loops
- [ ] ğŸŒ± Model Checkpointing

### ML Libraries
- [ ] ğŸŒ± Hugging Face Transformers
- [ ] ğŸŒ± Weights & Biases (Logging)
- [ ] ğŸŒ± Ray/RLlib (for RL)
- [ ] ğŸŒ± LangChain (for LLM apps)

## Research Skills
### Literature Review
- [ ] ğŸŒ± Finding Relevant Papers
- [ ] ğŸŒ± Reading Research Papers
- [ ] ğŸŒ± Summarizing Key Ideas
- [ ] ğŸŒ± Critical Analysis

### Experimentation
- [ ] ğŸŒ± Hypothesis Formation
- [ ] ğŸŒ± Experimental Design
- [ ] ğŸŒ± Ablation Studies
- [ ] ğŸŒ± Statistical Significance

### Scientific Writing
- [ ] ğŸŒ± Abstract Writing
- [ ] ğŸŒ± Related Work Section
- [ ] ğŸŒ± Methodology Description
- [ ] ğŸŒ± Results Presentation
- [ ] ğŸŒ± LaTeX Basics

## Project-Specific Skills
### Jailbreak Research
- [ ] ğŸŒ± Safety Evaluation Metrics
- [ ] ğŸŒ± Attack Success Measurement

### Implementation
- [ ] ğŸŒ± Setting up LLM APIs
- [ ] ğŸŒ± Prompt Template Design
- [ ] ğŸŒ± Batch Generation
- [ ] ğŸŒ± Result Analysis

---

## Update Log
### 2025-07-14
- Initial progress tracker created
- All topics marked as ğŸŒ± Novice to start

### How to Update
When you learn something new or improve your understanding:
1. Change the emoji to reflect new proficiency level
2. Add date and what you learned in the update log
3. This helps track learning journey and adjust guidance

### Next Focus Areas
Based on project needs, prioritize:
1. Python basics if not comfortable
2. Understanding jailbreaking conceptually
3. Basic neural networks
4. Simple RL concepts