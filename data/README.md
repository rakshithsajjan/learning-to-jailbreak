# Data Directory

This directory contains all datasets, prompts, and examples used in the jailbreak RL project.

## Directory Structure

### `/prompts/`
- **baseline/**: Rule-based and template prompts
- **gcg/**: GCG-generated adversarial suffixes
- **pair/**: PAIR conversation-based prompts
- **rl_generated/**: Prompts generated by RL agents
- **templates/**: Reusable prompt templates

### `/datasets/`
- **training/**: Data for training RL agents
- **evaluation/**: Test sets for measuring performance
- **benchmarks/**: Standard datasets (HarmBench, AdvBench)
- **raw/**: Original, unprocessed data

### `/examples/`
- **successful_attacks/**: Examples of successful jailbreaks
- **failed_attacks/**: Examples of failed attempts
- **edge_cases/**: Interesting or unusual cases
- **demonstrations/**: Examples for tutorials and documentation

## Data Sources

### Jailbreak Prompts
- Manual jailbreaks from research papers
- Community-sourced examples (with appropriate filtering)
- Generated examples from baseline methods

### Target Behaviors
- Harmful content categories
- Safety-relevant scenarios
- Edge cases and corner cases

## Data Guidelines

### Ethical Considerations
- All data used for defensive research only
- No deployment of harmful content
- Proper anonymization and filtering
- Coordination with model providers

### Quality Standards
- Verify prompt effectiveness
- Document data sources
- Maintain version control
- Regular quality audits

## Usage Examples

```python
# Load training prompts
from src.utils.data_loader import load_prompts
prompts = load_prompts('data/prompts/baseline/')

# Load evaluation dataset
from src.utils.data_loader import load_dataset
eval_data = load_dataset('data/datasets/evaluation/harmBench.json')
```